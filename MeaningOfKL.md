https://mathtod.online/@yoriyuki/738909
【Sanovの定理のことは知りませんでした。面白かったですが、これを持ってして損失関数にKL情報量を使う*べき*とは言えないと思いました。普通は情報利得と結びつけて理解すると思うのですが、これでは不十分というお考えなのですよね。】

「情報利得」の定義をどうしているか不明なのですが、

* 情報量を確率の対数の $-1$ 倍と定義する。

* 確率分布 $q(x)$ のもとで、確率分布 $q(x)$ の平均情報量$$
I(q) = \int q(x)(-\log q(x))\,dx
$$より確率分布 $p(x)$ の平均情報量$$
G(q||p) = \int q(x)(-\log p(x))\,dx
$$がどれだけ増えたか$$
D(q||p) = G(q||p) - I(q)\\
=\int q(x)\log\frac{q(x)}{p(x)}\,dx
$$を情報利得と呼ぶ、と定義する。

というスタイルだと、どうしてこれがモデルの確率分布 $p(x)$ による真の確率分布 $q(x)$ の予測精度の指標になるのかが不明の説明の仕方になってしまいます。

確率の対数の $-1$ 倍を「情報量」と呼ぶことの適切さは、文字 $x$ の出現確率が確率分布 $q(x)$ (有限離散分布とする)に従うファイルの $b$ 種類の文字による可逆圧縮の平均符号長の下限が対数の底が $b$ の場合の $I(q)$ であり(これは非自明)、 $x$ の $b$ 種類の文字による符号化の長さ $l(x) = -\log_b p(x)$ に従う可逆圧縮の平均符号長が $G(q||p)$ になること(およびMcmillanの不等式 $\sum_x p(x)\leqq 1$)を知っていれば、「可逆圧縮サイズ＝情報量」というイメージで納得できます。

しかし、これでも「予測精度の指標」からはかなり遠いイメージであり、このような説明でKL情報量が予測精度の指標であることに納得するのは無理でしょう。このこともすでに次のリンク先で指摘済み。

https://mathtod.online/@genkuroki/738808

予測が外れた場合の損失の大きさは応用目的によって変えるべきであることは当然です。だから、そういう意味ではKullback-Leibler情報量を損失函数として採用する必要はありません。以下ではこれを当然とした上での解説。

AlphaGoの初期バージョンにおける人間の強者が打った手の予測精度は55%程度でした。すなわち、AlphaGoの初期バージョンが確率的に生成する手が人間の強者の手と一致する確率は55%程度だった。

本質的に同じ別の話。簡単のため、明日、雨が降ることがすでに物理的に確定していたとしましょう。そのときに、確率 $p$ で「明日雨が降る」という目が出るサイコロを振って明日雨が降るかどうかを予想したときに、予想が当たる確率は $p$ です。この $p$ を予測精度の指標として使うことに異論はないでしょう。

Sanovの定理はKullback-Leibler情報量の $-1$ 倍の指数函数がこの意味での予測精度 $p$ の一般化になっていることを意味しています。

実際、確率 $1$ で雨が降る真の確率分布 $q$ と降雨確率を $p$ とする確率分布 $p$ (同じ記号で書く)に関するKL情報量は$$
D(q||p) = 1\log\frac{1}{p}+0\log\frac{0}{1-p}\\
=-\log p
$$になり、これの $-1$ 倍の $\exp$ は $p$ になります。

そして、以上と完全に同一のケースが $n$ 回あるとき、サイコロを振って得られる予測が $n$ 回とも全部当たる確率は$$
p^n = \exp(-n(-\log p)) = \exp(-n D(q||p))
$$になります。

Sanovの定理の主張は、「確率分布 $p$ にしたがった $n$ の独立試行の結果得られる経験分布が確率分布 $q$ にほぼ一致する確率は$$
  \exp(-n D(q||p) + o(n))
$$と振る舞う」です。誤差項 $o(n)$ が追加されている点を除けば、上のサイコロによる降雨予想が当たる確率の話と同じことになっているわけです。

KL情報量 $D(q||p)$ はモデルの確率分布 $p$ による真の確率分布 $q$ のシミュレーションにおいてぼろが出る速さを意味していると解釈できる。  

こういう類のことさえ知らずに、「KL情報量を予測精度の指標として使える」と言えるはずがないというのが、私が繰り返し強調していることです。
